{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport matplotlib.pyplot as plt \nimport cv2\nimport glob\n\n# This is a bit of magic to make matplotlib figures appear inline in the notebook\n# rather than in a new window.\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (19.0, 17.0) # set default size of plots\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\n\n# Some more magic so that the notebook will reload external python modules;\n# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n%load_ext autoreload\n%autoreload 2\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"# Data preprocessing "},{"metadata":{"trusted":true,"_uuid":"a25da61700ad069406e07eb00edde3b1c75b670e"},"cell_type":"code","source":"data_dir = r'/kaggle/input/dataset/dataset/'\nclasses = ['broadleaf', 'grass', 'soil', 'soybean'] \n\nnum_file = 1100 \nall_files = [] \nnum_data =num_file*len(classes)\nY = np.zeros(num_data)\n\n\nfor i, cls in enumerate(classes):\n    all_files += [f for f in glob.glob(data_dir+cls+'/*.tif')][:num_file]\n    Y[i*num_file:(i+1)*num_file] = i # label all classes with int [0.. len(classes)]\n\n    \n# Image dimension\nim_width = 200\nim_height = 200 \nim_channel = 3\ndim = im_width * im_height * im_channel\n\nX = np.ndarray(shape=(num_data, im_width, im_height, im_channel), dtype=np.float64)\n\nfor idx, file in enumerate(all_files):\n    X[idx] = cv2.resize(cv2.imread(file), (im_width, im_height))\n\nX_train = np.empty(shape=(4000,im_width, im_height, im_channel), dtype=np.float64)\nX_val = np.empty(shape=(200,im_width, im_height, im_channel), dtype=np.float64)\nX_test = np.empty(shape=(200,im_width, im_height, im_channel), dtype=np.float64)\n\ny_train = np.empty(4000)\ny_val = np.empty(200)\ny_test = np.empty(200) \n\nfor i, cls in enumerate(classes): \n    X_test[50*i:50*(i+1)] = X[np.where(Y == i)[0][:50]]\n    X_val[50*i:50*(i+1)] = X[np.where(Y == i)[0][50:100]]\n    X_train[1000*i:1000*(i+1)] = X[np.where(Y == i)[0][100:]]\n    \n    y_test[50*i:50*(i+1)] = i\n    y_val[50*i:50*(i+1)] = i\n    y_train[1000*i:1000*(i+1)] = i\n    \ndel Y \ndel X\n\n# Extract features \n#Shuffle training index\ntrain_idxs = np.random.permutation(X_train.shape[0])\ny_train  = y_train[train_idxs].astype(int)\nX_train = X_train[train_idxs]\n\nX_train = np.reshape(X_train, (X_train.shape[0], -1)).astype('float64')\nX_test = np.reshape(X_test, (X_test.shape[0], -1)).astype('float64')\nX_val = np.reshape(X_val, (X_val.shape[0], -1)).astype('float64')\n\nX_tiny = X_train[100:110].astype('float64')\ny_tiny = y_train[100:110].astype(int)\nnum_dev = 500\n\nX_dev = X_train[0:num_dev].astype('float64')\ny_dev = y_train[0:num_dev].astype(int)\nprint(\"X_train shape\", X_train.shape, \"| y_train shape:\", y_train.shape)\nprint(\"X_test shape\", X_test.shape, \"| y_test shape:\", y_test.shape)\nprint(\"X_val shape\", X_val.shape, \"| y_val shape:\", y_val.shape)\nprint(\"X_dev shape\", X_dev.shape, \"| y_dev shape:\", y_dev.shape)\nprint(\"X_tiny shape\", X_tiny.shape, \"| y_tiny shape:\", y_tiny.shape)\n\n#Subtract out the mean image \n#first: compute the mean image\n# mean_image = np.mean(X_train, axis=0) #axis=0. stack horizontally\nmean_image = 128\n#Second subtract the mean image from train and test data \nX_train -= mean_image\nX_val -= mean_image \nX_test -= mean_image\nX_dev -= mean_image\nX_tiny -= mean_image\n\n#Third append the bias dimension using linear algebra trick\n#Not for net\n# X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n# X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n# X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n# X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n# X_tiny = np.hstack([X_tiny, np.ones((X_tiny.shape[0], 1))])\n\nprint('=====STACK BIAS term=====')\nprint(\"X_train shape\", X_train.shape)\nprint(\"X_test shape\", X_test.shape)\nprint(\"X_val shape\", X_val.shape)\nprint(\"X_dev shape\", X_dev.shape)\nprint(\"X_tiny shape\", X_tiny.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2dc1992c239ee1e6f4cd60d7df5e6dcd2ddafafe"},"cell_type":"code","source":"# Visualize some images \n# Make sure that everything when OK\nclasses = ['broadleaf', 'grass', 'soil', 'soybean']\nn_class = len(classes)\nsamples_per_class = 4\n\n\nfor y, cls in enumerate(classes):\n    idxes = np.flatnonzero(y == y_train)\n    idxes = np.random.choice(idxes, samples_per_class, replace = False)\n    for i, idx in enumerate(idxes):\n        plt_idx = i * n_class + y + 1\n        plt.subplot(samples_per_class,n_class, plt_idx)\n        plt.imshow(X_train[idx].reshape(im_width, im_height, im_channel).astype('uint8'))\n        if(i==0): plt.title(cls)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class NeuralNetwork:\n  hidden_size = 200\n  input_size = im_width * im_height * im_channel\n  output_size = n_class\n\n  \"\"\"\n  w1: first layer weight\n  w2: second layer weight\n  \"\"\"\n  w1 = 1e-3 * np.random.randn(input_size, hidden_size)\n  b1 = np.zeros(hidden_size)\n  w2 = 1e-3 * np.random.randn(hidden_size, output_size)\n  b2 = np.zeros(output_size)\n    \n  alpha = 1e-5\n  batch_size = 100\n    \n  epochs = 500\n  \n  def train(self, X, Y, X_val, Y_val):\n    N, D = X.shape\n    N_val = X_val.shape[0]\n    iteration_per_epoch = max(N / self.batch_size, 1)\n\n    loss_hist = []\n    train_acc_hist = []\n    val_acc_hist = []\n    \n    for it in range(self.epochs):\n      sampling = np.random.choice(np.arange(N), self.batch_size, replace=False) # Create random array data\n\n      # Getting batches for x and y\n      X_batch = X[sampling]\n      Y_batch = Y[sampling]\n\n      loss, grads = self.loss(X_batch, Y=Y_batch)\n      loss_hist.append(loss)\n\n      # Make the model learning and reshape the parameters of the network\n      self.w1 += -1.0 * self.alpha * grads['w1']\n      self.b1 += -1.0 * self.alpha * grads['b1']\n      self.w2 += -1.0 * self.alpha * grads['w2']\n      self.b2 += -1.0 * self.alpha * grads['b2']\n\n      if it % 10 == 0:\n        print('iteration: %d / %d | Loss: %f' % (it, self.epochs, loss))\n    \n      if it % iteration_per_epoch == 0:\n        train_acc = (self.predict(X_batch) == Y_batch).mean()\n        val_acc = (self.predict(X_val) == Y_val).mean()\n        train_acc_hist.append(train_acc)\n        val_acc_hist.append(val_acc)\n\n        self.alpha *= 0.95\n    \n    return {\n        'loss_hist': loss_hist,\n        'train_acc_hist': train_acc_hist,\n        'val_acc_hist': val_acc_hist\n    }\n\n  def relu(self, z):\n    return np.maximum(0, z)\n  \n  def predict(self, X):\n    Y_pred = None\n    layer1 = self.relu(X.dot(self.w1) + self.b1)\n    scores = layer1.dot(self.w2) + self.b2\n    Y_pred = np.argmax(scores, axis=1)\n    return Y_pred\n\n  def loss(self, X, Y = None):\n    N, D = X.shape\n\n    # Calculate the loss of our layer1\n    layer1 = self.relu(X.dot(self.w1) + self.b1)\n    scores = layer1.dot(self.w2) + self.b2\n\n    if (Y is None):\n      return scores\n\n    # Calculate the actual loss\n    scores -= scores.max()\n    scores = np.exp(scores)\n    scores_sumexp = np.sum(scores, axis=1)\n    softmax = scores / scores_sumexp.reshape(N, 1)\n    loss = -1.0 * np.sum(np.log(softmax[range(N), Y]))\n    loss /= N\n\n    grads = {}\n    correct_class_scores = scores[range(N), Y]\n    softmax[range(N), Y] = -1.0 * (scores_sumexp - correct_class_scores) / scores_sumexp\n    softmax /= N\n\n    grads['w2'] = layer1.T.dot(softmax)\n    grads['b2'] = np.sum(softmax, axis=0)\n\n    hidden = softmax.dot(self.w2.T)\n\n    grads['w1'] = X.T.dot(hidden)\n    grads['b1'] = np.sum(hidden, axis=0)\n\n    return loss, grads","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nnn = NeuralNetwork()\nstats = nn.train(X_train, y_train, X_val, y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"44046c8077792973cc5d42bf527de23a6103529c"},"cell_type":"code","source":"# plot loss history and train/ validation accuracies history\nplt.rcParams['figure.figsize'] = (15.0, 15.0) # set default size of plots\nplt.subplot(2,1,1) \nplt.plot(stats['loss_hist'])\nplt.title('Loss History')\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\n\nplt.subplot(2,1,2)\nplt.plot(stats['train_acc_hist'], label='train')\nplt.plot(stats['val_acc_hist'], label='val')\nplt.xlabel('Epoch')\nplt.ylabel('Classfication Accuracies')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f99bff1fc33a8a6d59f0b055463d5076a638c31b"},"cell_type":"code","source":"print((nn.predict(X_test) == y_test).mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d0dba50a10485c4a70b7d9fdb973108bf83ee88"},"cell_type":"code","source":"index = np.flatnonzero(0 == y_train)\nindex = np.random.choice(index, 1, replace=False)\nprediction = nn.predict(X_train[index])\n\nplt.subplot(1, 2, 1)\nplt.title('{} | {}'.format(classes[0], classes[prediction[0]]))\nplt.imshow(X_train[index].reshape(im_width, im_height, im_channel).astype('uint8'))\n\nindex = np.flatnonzero(1 == y_train)\nindex = np.random.choice(index, 1, replace=False)\nprediction = nn.predict(X_train[index])\n\nplt.subplot(1, 2, 2)\nplt.title('{} | {}'.format(classes[1], classes[prediction[0]]))\nplt.imshow(X_train[index].reshape(im_width, im_height, im_channel).astype('uint8'))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}